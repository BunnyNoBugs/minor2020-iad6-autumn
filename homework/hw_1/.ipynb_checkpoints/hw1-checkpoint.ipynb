{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №1\n",
    "\n",
    "В этом домашнем задании вы познакомитесь с pytorch сами и сможете попрактиковаться в его применении. \n",
    "\n",
    "#### План:\n",
    "\n",
    "1. Простейшие операции в pytorch\n",
    "2. Пишем Adam и применяем его к ручной модели\n",
    "3. Обучаем свою первую нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простейшие операции на pytorch (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Cоздайте два случайных тензора (двумерных, не квадратных):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 5)\n",
    "y = torch.rand(4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Умножьте их друг на друга, результат запишите в третий тензор без использования оператора `=`, для создания третьего тензора предлагается использовать `torch.empty`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1045, 0.1810, 0.3261, 0.0491, 0.6154],\n",
       "        [0.0947, 0.0709, 0.0537, 0.0381, 0.0173],\n",
       "        [0.2170, 0.2981, 0.2112, 0.1255, 0.0872],\n",
       "        [0.6735, 0.1738, 0.2266, 0.0840, 0.2253]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.empty(4, 5)\n",
    "torch.mul(x, y, out=z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor([-1, -1])\n",
    "# F.relu(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[l == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Реализуйте ReLU используя только pytorch, примените его к тензору `x` (запрещено использование модулей torch.nn и его подмодулей, а также функции torch.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-de72d55894ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-de72d55894ef>\u001b[0m in \u001b[0;36mrelu_forward\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "def relu_forward(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "assert torch.all(F.relu(x) == relu_forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Сделайте тоже самое c ELU (запрещено использование модулей torch.nn и его подмодулей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_forward(x):\n",
    "    pass\n",
    "\n",
    "assert torch.allclose(\n",
    "    F.elu(x),\n",
    "    elu_forward(x), \n",
    "    1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: LeakyReLU (запрещено использование модулей torch.nn и его подмодулей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu_forward(x, alpha):\n",
    "    pass\n",
    "\n",
    "assert torch.all(F.leaky_relu(x, 0.01) == lrelu_forward(x, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Теперь перейдем к немного более современным функциям активаций, например Mish, напомним как она выглядит:\n",
    "\n",
    "$$x * tanh(ln(1+e^x))$$\n",
    "\n",
    "(запрещено использование модулей torch.nn и его подмодулей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    mish(torch.tensor([1, 1, 1], dtype=torch.float32)), \n",
    "    torch.tensor([0.8651, 0.8651, 0.8651]), \n",
    "    atol=1e-4\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    mish(torch.tensor([0.6376, 0.4021, 0.6656, 0.3726], dtype=torch.float64)), \n",
    "    torch.tensor([0.5014, 0.2908, 0.5280, 0.2663], dtype=torch.float64), \n",
    "    atol=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Теперь реализуем swish, напомним как она выглядит:\n",
    "\n",
    "$$x * \\sigma(x)$$\n",
    "\n",
    "(запрещено использование модулей torch.nn и его подмодулей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    swish(torch.tensor([1, 1, 1], dtype=torch.float32)), \n",
    "    torch.tensor([0.7311, 0.7311, 0.7311]), \n",
    "    atol=1e-4\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    swish(torch.tensor([0.6376, 0.4021, 0.6656, 0.3726], dtype=torch.float64)), \n",
    "    torch.tensor([0.4171, 0.2409, 0.4396, 0.2206], dtype=torch.float64), \n",
    "    atol=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пишем Adam и применяем его к логистической регрессии (4 балла)\n",
    "\n",
    "\n",
    "В данной секции вам нужно сделать две вещи: \n",
    "\n",
    "1. Написать свой собственный оптимизатор подобно тому, который мы писали на семинаре\n",
    "2. Обучить логистическую регрессию побатчево на картинках из датасета \n",
    "\n",
    "\n",
    "#### Adam\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g &=& \\frac{1}{m}\\nabla_w \\sum_i L(f(x_{i};w), y_{i}) \\\\\n",
    "m &=& \\beta_1 m + (1 - \\beta_1) g \\\\\n",
    "v &=& \\beta_2 v + (1 - \\beta_2) diag(gg^{T}) \\\\\n",
    "\\hat{m} &=& \\frac{m}{1 - \\beta_1^{t}} \\\\\n",
    "\\hat{v} &=& \\frac{v}{1 - \\beta_2^{t}} \\\\\n",
    "w &=& w - \\frac{\\eta}{\\sqrt{\\hat{v} + \\epsilon}} \\odot \\hat{m}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# абстрактный класс, не обращайте внимания\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class InClassOptimizer(Optimizer):\n",
    "    def step(self):\n",
    "        \"\"\"Perform single optimization step.\"\"\"\n",
    "        with torch.no_grad(): # выключим градиенты\n",
    "            for group in self.param_groups:\n",
    "                self._group_step(group)\n",
    "\n",
    "    def _group_step(self, group):\n",
    "        # group ~ dict[str, ...]\n",
    "        \"\"\"\n",
    "        Private helper function to perform\n",
    "        single optimization step on model parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Напишите свою реализацию Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(InClassOptimizer):\n",
    "    def __init__(self, params, lr=0.001, eps=1e-13, beta_1=0.9, beta_2=0.999):\n",
    "        defaults = dict(lr=lr, eps=eps, beta_1=beta_1, beta_2=beta_2)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def _group_step(self, group):\n",
    "        # One group contains information about values passed in init\n",
    "        # and model parameters to update\n",
    "        lr = group['lr']\n",
    "        eps = group['eps']\n",
    "        beta_1 = group['beta_1']\n",
    "        beta_2 = group['beta_2']\n",
    "        for param in filter(lambda x: x.grad is not None, group['params']):\n",
    "            pass\n",
    "\n",
    "    def _get_adam_buffer(self, param):\n",
    "        \"\"\"\n",
    "        Get accumulated gradients for Adam.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : `torch.Tensor`, required\n",
    "            Model parameter to get accumulated gradeints for Adagrad.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Accumulated Adam gradients for parameter.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _init_adam_buffer(self, param):\n",
    "        \"\"\"\n",
    "        Initialize accumulated gradeints for adam.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param : `torch.Tensor`, required\n",
    "            Model parameter to get accumulated gradeints for adam.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Создайте параметры для обучения логистической регрессии, сделаем Xavier ициализацию, которая выглядит следующим образом: \n",
    "\n",
    "$$w \\sim U[-\\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{in} + n_{out}}}]$$\n",
    "\n",
    "где: \n",
    "\n",
    "* $n_{in}$ -- размер входа (в нейронах) \n",
    "* $n_{out}$ -- размер выхода (в нейронах)\n",
    "\n",
    "Подумайте над выбором $n_{in}$ и $n_{out}$ самостоятельно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = # your code here\n",
    "intercept = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam([weights, intercept])\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и создадим даталоадеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    './data',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "train_dataloader = # your code here\n",
    "\n",
    "fashion_mnist_eval = torchvision.datasets.FashionMNIST(\n",
    "    './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "eval_dataloader = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию для тренировки логистической регрессии, она должна: \n",
    "    * Делать предсказания \n",
    "    * Считать лосс \n",
    "    * Подсчитывать градиенты\n",
    "    * Делать шаг оптимизации\n",
    "    * Обнулять посчитанные градиенты\n",
    "    * Считать метрики \n",
    "    * Возвращать полученные метрики\n",
    "    \n",
    "После этого предусмотрите возможность визуализировать метрики, чтобы нарисовать картинки, а именно от вас требуется визуализировать: \n",
    "\n",
    "    * Зависимость лосса от количества итераций\n",
    "    * Зависимость доли правильных ответов от количества итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(weights, bias, batch, loss, optimizer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 100):\n",
    "    for batch in train_dataloader:\n",
    "        metrics = train_logistic_regression(weights, bias, batch, loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопросы к секции: \n",
    "\n",
    "* Своими словами и без математики объясните благодаря чему Adam дает несмещенную оценку на квадрат градиента\n",
    "* Когда модель начала переобучаться? Как вы это поняли? Сделайте визуализацию и докажите свою точку зрения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моя первая нейросеть (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной секции вам нужно сделать следующие вещи: \n",
    "\n",
    "* Реализовать три разных архитектуры нейросетей. Эти архитектуры должны принципиально отличаться друг от друга. Разрешается одной из архитекур брать полностью полносвязную модель. Остальные две должны быть сверточными и сильно отличаться друг от друга. К примеру, одна из таких архитектур может быть VGG подобная сеть, а другая ResNet подобная архитектура. \n",
    "\n",
    "* Написать цикл для обучения которым можно обучать все три модели без изменений кода\n",
    "\n",
    "* Попробовать каждую модель с двумя оптимизаторами: RMSprop и SGD with momentum\n",
    "\n",
    "* Визуализировать результаты перфоманса каждой модели (две метрики минимум для каждого сетапа, например, лосс и долю правильных ответов). В данном пункте мы ждем от вас визуализацию зависимости метрики от номера итерации обучения.\n",
    "\n",
    "* Сделать выводы какие были модели были лучше и как вы думаете почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = nn.Sequential(\n",
    "    # your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_model = torch.nn.ModuleDict({\n",
    "    # your code here\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and visualize and write summary down there"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
